{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path='d:/My Python/'\n",
    "path = 'C:/Users/HSIU/Desktop/108-1/python_ntu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果正向文章與負向文章，以文字檔形式，分別放在不同的資料夾，\n",
    "def text():\n",
    "     f1 = open(path+'pos.txt','r',encoding='utf-8') \n",
    "     f2 = open(path+'neg.txt','r',encoding='utf-8')\n",
    "     line1 = f1.readline()\n",
    "     line2 = f2.readline()\n",
    "     str = ''\n",
    "     while line1:\n",
    "         str += line1\n",
    "         line1 = f1.readline()\n",
    "     while line2:\n",
    "         str += line2\n",
    "         line2 = f2.readline()\n",
    "     f1.close()\n",
    "     f2.close()\n",
    "     return str\n",
    "try:\n",
    "    text=text()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function text at 0x000001558D9D5AE8>\n"
     ]
    }
   ],
   "source": [
    "# 正向負向文本併在一起，尚未斷詞\n",
    "print (text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "import nltk\n",
    "from nltk.collocations import  BigramCollocationFinder\n",
    "from nltk.metrics import  BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結巴斷詞\n",
    "import jieba\n",
    "def read_file(filename):\n",
    "     stop = [line.strip() for line in  open(path+'stop.txt','r',encoding='utf-8').readlines()]  #停用词\n",
    "     f = open(filename,'r',encoding='utf-8')\n",
    "     line = f.readline()\n",
    "     str = []\n",
    "     # 每行為一篇文章\n",
    "     while line:\n",
    "         s = line.split('\\n')   # split based on line            \n",
    "         fenci = jieba.cut(s[0],cut_all=False)  #False默认值：精准模式\n",
    "         str.append(list(set(fenci)-set(stop)))  # 去除停用字，加入文字袋 list\n",
    "         line = f.readline()\n",
    "     return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import  FreqDist,ConditionalFreqDist\n",
    "from nltk.metrics import  BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每篇文章，取前面最有鑑別力的幾個字詞\n",
    "def jieba_feature(number):   \n",
    "     posWords = []\n",
    "     negWords = []\n",
    "     # 讀取檔案，每行為一篇文章，結巴斷詞，存成 list\n",
    "     for items in read_file(path+'/pos.txt'):\n",
    "         for item in items:\n",
    "            posWords.append(item)\n",
    "     for items in read_file(path+'/neg.txt'):\n",
    "         for item in items:\n",
    "            negWords.append(item)\n",
    "            \n",
    "     # 宣告函數\n",
    "     word_fd = FreqDist() # 統計所有字詞的出現次數\n",
    "     cond_word_fd = ConditionalFreqDist() #條件統計字頻\n",
    "        \n",
    "     # 正向貼文的字詞頻率\n",
    "     for word in posWords:\n",
    "         word_fd[word] += 1\n",
    "         cond_word_fd['pos'][word] += 1\n",
    "            \n",
    "     # 負向貼文的字詞頻率\n",
    "     for word in negWords:\n",
    "         word_fd[word] += 1\n",
    "         cond_word_fd['neg'][word] += 1\n",
    "            \n",
    "     # 正向字詞個數 \n",
    "     pos_word_count = cond_word_fd['pos'].N() \n",
    "     # 負向字詞個數   \n",
    "     neg_word_count = cond_word_fd['neg'].N() \n",
    "     # 所有字詞個數   \n",
    "     total_word_count = pos_word_count + neg_word_count\n",
    "\n",
    "     word_scores = {} # 計算每個字詞的鑑別力\n",
    "     for word, freq in word_fd.items():\n",
    "         #计算 pos, neg 的卡方统计量   \n",
    "         pos_score = BigramAssocMeasures.chi_sq(cond_word_fd['pos'][word],  (freq, pos_word_count), total_word_count)         \n",
    "         neg_score = BigramAssocMeasures.chi_sq(cond_word_fd['neg'][word],  (freq, neg_word_count), total_word_count) \n",
    "         #一个词的信息量等于积极卡方统计量加上消极卡方统计量\n",
    "         word_scores[word] = pos_score + neg_score \n",
    "        \n",
    "     #把词按信息量倒序排序。選取 number特征维度\n",
    "     best_vals = sorted(word_scores.items(), key=lambda item:item[1],  reverse=True)[:number]     \n",
    "     best_words = set([w for w,s in best_vals])\n",
    "    \n",
    "     # 傳回 dictionary，最有鑑別力的 number 個字詞 \n",
    "     return dict([(word, True) for word in best_words])        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填入情感詞\n",
    "def build_features():       \n",
    "     # dictionary, 各篇文章的字詞(key)，是否出現 True, False (value)   \n",
    "     feature = jieba_feature(300) #结巴分词，取前面 300 個最重要的字詞     \n",
    "    \n",
    "     # list 填入情感詞 \n",
    "     posFeatures = []\n",
    "     for items in read_file(path+'/pos.txt'):\n",
    "         a = {}\n",
    "         for item in items:\n",
    "            if item in feature.keys():\n",
    "                a[item]='True'\n",
    "         # 正向文章填入 pos       \n",
    "         posWords = [a,'pos'] \n",
    "         posFeatures.append(posWords)    \n",
    "        \n",
    "     negFeatures = []    \n",
    "     for items in read_file(path+'/neg.txt'):\n",
    "         a = {}\n",
    "         for item in items:\n",
    "            if item in feature.keys():\n",
    "                a[item]='True'\n",
    "         negWords = [a,'neg'] \n",
    "         negFeatures.append(negWords)\n",
    "            \n",
    "     # 傳回 list，含字詞及情感詞       \n",
    "     return posFeatures,negFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/HSIU/Desktop/108-1/python_ntustop.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-fd61f0847ca3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mposFeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnegFeatures\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mbuild_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-da47216736f9>\u001b[0m in \u001b[0;36mbuild_features\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbuild_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m      \u001b[1;31m# dictionary, 各篇文章的字詞(key)，是否出現 True, False (value)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m      \u001b[0mfeature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjieba_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#结巴分词，取前面 300 個最重要的字詞\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m      \u001b[1;31m# list 填入情感詞\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-cac51998d7df>\u001b[0m in \u001b[0;36mjieba_feature\u001b[1;34m(number)\u001b[0m\n\u001b[0;32m      4\u001b[0m      \u001b[0mnegWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m      \u001b[1;31m# 讀取檔案，每行為一篇文章，結巴斷詞，存成 list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m      \u001b[1;32mfor\u001b[0m \u001b[0mitems\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/pos.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m          \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mposWords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-180ede924c4f>\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m      \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m  \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'stop.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m#停用词\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m      \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m      \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/HSIU/Desktop/108-1/python_ntustop.txt'"
     ]
    }
   ],
   "source": [
    "posFeatures,negFeatures =  build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'喜歡': 'True'}, 'pos'],\n",
       " [{'。': 'True', '高興': 'True'}, 'pos'],\n",
       " [{'。': 'True', '太棒了': 'True', '真是': 'True'}, 'pos'],\n",
       " [{'。': 'True', '好': 'True', '我覺': 'True', '心情': 'True'}, 'pos'],\n",
       " [{'音樂會': 'True', '令人': 'True', '。': 'True', '昨天晚上': 'True', '感動': 'True'},\n",
       "  'pos'],\n",
       " [{'最要': 'True', '。': 'True', '好': 'True', '朋友': 'True'}, 'pos']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'搭公車': 'True', '。': 'True', '討厭': 'True'}, 'neg'],\n",
       " [{'太': 'True',\n",
       "   '可怕': 'True',\n",
       "   '。': 'True',\n",
       "   '經歷實': 'True',\n",
       "   '在': 'True',\n",
       "   '了': 'True'},\n",
       "  'neg'],\n",
       " [{'累': 'True', '我覺': 'True', '。': 'True'}, 'neg'],\n",
       " [{'期待': 'True', '研討會': 'True', '。': 'True', '參加': 'True', '不': 'True'},\n",
       "  'neg'],\n",
       " [{'喜歡': 'True', '。': 'True', '不': 'True'}, 'neg'],\n",
       " [{'競爭': 'True', '。': 'True', '手': 'True', '對': 'True'}, 'neg']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比較原來之文本\n",
    "* pos\n",
    "* 我喜歡車子\n",
    "* 早上我很高興。\n",
    "* 他真是太棒了。\n",
    "* 早上我覺得心情很好。\n",
    "* 昨天晚上的音樂會令人感動。\n",
    "* neg\n",
    "* 他是我最要好的朋友。我討厭搭公車。\n",
    "* 這個經歷實在太可怕了。\n",
    "* 早上我覺得很累。\n",
    "* 我一點也不期待參加這個研討會。\n",
    "* 我不喜歡他。\n",
    "* 他是我的競爭對手。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將含有情感詞的文章，分為訓練組與測試組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "#把文本的排列随机化 \n",
    "shuffle(posFeatures) \n",
    "shuffle(negFeatures) \n",
    "# 設定 訓練集 [2:] 共 8 筆\n",
    "train =  posFeatures[2:]+negFeatures[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 測試集 [:2] 共4筆\n",
    "test = posFeatures[:2]+negFeatures[:2]\n",
    "\n",
    "#分离测试集合的数据和标签，便于验证和测试\n",
    "data,tag = zip(*test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'。': 'True', '高興': 'True'}, 'pos'],\n",
       " [{'。': 'True', '太棒了': 'True', '真是': 'True'}, 'pos'],\n",
       " [{'。': 'True', '好': 'True', '我覺': 'True', '心情': 'True'}, 'pos'],\n",
       " [{'音樂會': 'True', '令人': 'True', '。': 'True', '昨天晚上': 'True', '感動': 'True'},\n",
       "  'pos'],\n",
       " [{'累': 'True', '我覺': 'True', '。': 'True'}, 'neg'],\n",
       " [{'競爭': 'True', '。': 'True', '手': 'True', '對': 'True'}, 'neg'],\n",
       " [{'喜歡': 'True', '。': 'True', '不': 'True'}, 'neg'],\n",
       " [{'太': 'True',\n",
       "   '可怕': 'True',\n",
       "   '。': 'True',\n",
       "   '經歷實': 'True',\n",
       "   '在': 'True',\n",
       "   '了': 'True'},\n",
       "  'neg']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'喜歡': 'True'}, 'pos'],\n",
       " [{'最要': 'True', '。': 'True', '好': 'True', '朋友': 'True'}, 'pos'],\n",
       " [{'搭公車': 'True', '。': 'True', '討厭': 'True'}, 'neg'],\n",
       " [{'期待': 'True', '研討會': 'True', '。': 'True', '參加': 'True', '不': 'True'},\n",
       "  'neg']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械學習，計算準確度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'。': 'True', '高興': 'True'}, 'pos'],\n",
       " [{'。': 'True', '太棒了': 'True', '真是': 'True'}, 'pos'],\n",
       " [{'。': 'True', '好': 'True', '我覺': 'True', '心情': 'True'}, 'pos'],\n",
       " [{'音樂會': 'True', '令人': 'True', '。': 'True', '昨天晚上': 'True', '感動': 'True'},\n",
       "  'pos'],\n",
       " [{'累': 'True', '我覺': 'True', '。': 'True'}, 'neg'],\n",
       " [{'競爭': 'True', '。': 'True', '手': 'True', '對': 'True'}, 'neg'],\n",
       " [{'喜歡': 'True', '。': 'True', '不': 'True'}, 'neg'],\n",
       " [{'太': 'True',\n",
       "   '可怕': 'True',\n",
       "   '。': 'True',\n",
       "   '經歷實': 'True',\n",
       "   '在': 'True',\n",
       "   '了': 'True'},\n",
       "  'neg']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(classifier,train,data):\n",
    "     classifier = SklearnClassifier(classifier) \n",
    "     #训练分类器   \n",
    "     classifier.train(train) \n",
    "     # data,testing and predict  \n",
    "     pred = classifier.classify_many(data) #给出预测的标签\n",
    "     n = 0\n",
    "     s = len(pred)\n",
    "     for i in range(0,s):\n",
    "         if pred[i]==tag[i]:\n",
    "            n = n+1\n",
    "            print ('correct:', pred[i],test[i])\n",
    "         else:\n",
    "            print ('incorrect:',pred[i],test[i])\n",
    "            pass\n",
    "     return n/s, pred #分类器准确度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from nltk.classify.scikitlearn import  SklearnClassifier\n",
    "from sklearn.svm import SVC, LinearSVC,  NuSVC\n",
    "from sklearn.naive_bayes import  MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.metrics import  accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演算法1:  BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB-------------------\n",
      "incorrect: neg [{'喜歡': 'True'}, 'pos']\n",
      "correct: pos [{'最要': 'True', '。': 'True', '好': 'True', '朋友': 'True'}, 'pos']\n",
      "incorrect: pos [{'搭公車': 'True', '。': 'True', '討厭': 'True'}, 'neg']\n",
      "correct: neg [{'期待': 'True', '研討會': 'True', '。': 'True', '參加': 'True', '不': 'True'}, 'neg']\n",
      "accuracy is 0.5\n",
      "predict ['neg', 'pos', 'pos', 'neg']\n"
     ]
    }
   ],
   "source": [
    "print ('BernoulliNB-------------------')\n",
    "sco=score(BernoulliNB(),train,data)\n",
    "print('accuracy is', sco[0])\n",
    "print ('predict', sco[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演算法2: MultinomiaNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()-------------------\n",
      "incorrect: neg [{'喜歡': 'True'}, 'pos']\n",
      "correct: pos [{'最要': 'True', '。': 'True', '好': 'True', '朋友': 'True'}, 'pos']\n",
      "incorrect: pos [{'搭公車': 'True', '。': 'True', '討厭': 'True'}, 'neg']\n",
      "correct: neg [{'期待': 'True', '研討會': 'True', '。': 'True', '參加': 'True', '不': 'True'}, 'neg']\n",
      "accuracy is 0.5\n",
      "predict ['neg', 'pos', 'pos', 'neg']\n"
     ]
    }
   ],
   "source": [
    "print ('MultinomialNB()-------------------')\n",
    "sco=score(MultinomialNB(),train,data)\n",
    "print('accuracy is', sco[0])\n",
    "print ('predict', sco[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演算法3:LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression-------------------\n",
      "incorrect: neg [{'喜歡': 'True'}, 'pos']\n",
      "correct: pos [{'最要': 'True', '。': 'True', '好': 'True', '朋友': 'True'}, 'pos']\n",
      "incorrect: pos [{'搭公車': 'True', '。': 'True', '討厭': 'True'}, 'neg']\n",
      "correct: neg [{'期待': 'True', '研討會': 'True', '。': 'True', '參加': 'True', '不': 'True'}, 'neg']\n",
      "accuracy is 0.5\n",
      "predict ['neg', 'pos', 'pos', 'neg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brat\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print ('LogisticRegression-------------------')\n",
    "sco=score(LogisticRegression(),train,data)\n",
    "print('accuracy is', sco[0])\n",
    "print ('predict', sco[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演算法4:SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC-------------------\n",
      "incorrect: neg [{'喜歡': 'True'}, 'pos']\n",
      "correct: pos [{'最要': 'True', '。': 'True', '好': 'True', '朋友': 'True'}, 'pos']\n",
      "incorrect: pos [{'搭公車': 'True', '。': 'True', '討厭': 'True'}, 'neg']\n",
      "correct: neg [{'期待': 'True', '研討會': 'True', '。': 'True', '參加': 'True', '不': 'True'}, 'neg']\n",
      "accuracy is 0.5\n",
      "predict ['neg', 'pos', 'pos', 'neg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brat\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print ('SVC-------------------')\n",
    "sco=score(SVC(),train,data)\n",
    "print('accuracy is', sco[0])\n",
    "print ('predict', sco[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演算法5:LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC-------------------\n",
      "incorrect: neg [{'喜歡': 'True'}, 'pos']\n",
      "correct: pos [{'最要': 'True', '。': 'True', '好': 'True', '朋友': 'True'}, 'pos']\n",
      "incorrect: pos [{'搭公車': 'True', '。': 'True', '討厭': 'True'}, 'neg']\n",
      "correct: neg [{'期待': 'True', '研討會': 'True', '。': 'True', '參加': 'True', '不': 'True'}, 'neg']\n",
      "accuracy is 0.5\n",
      "predict ['neg', 'pos', 'pos', 'neg']\n"
     ]
    }
   ],
   "source": [
    "print ('LinearSVC-------------------')\n",
    "sco=score(LinearSVC(),train,data)\n",
    "print('accuracy is', sco[0])\n",
    "print ('predict', sco[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演算法6:NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC-------------------\n",
      "incorrect: neg [{'喜歡': 'True'}, 'pos']\n",
      "correct: pos [{'最要': 'True', '。': 'True', '好': 'True', '朋友': 'True'}, 'pos']\n",
      "incorrect: pos [{'搭公車': 'True', '。': 'True', '討厭': 'True'}, 'neg']\n",
      "correct: neg [{'期待': 'True', '研討會': 'True', '。': 'True', '參加': 'True', '不': 'True'}, 'neg']\n",
      "accuracy is 0.5\n",
      "predict ['neg', 'pos', 'pos', 'neg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brat\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print ('NuSVC-------------------')\n",
    "sco=score(NuSVC(),train,data)\n",
    "print('accuracy is', sco[0])\n",
    "print ('predict', sco[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
